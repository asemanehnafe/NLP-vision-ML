{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1133e90c-f118-40ad-bb97-c0760855e3e4",
      "metadata": {
        "id": "1133e90c-f118-40ad-bb97-c0760855e3e4",
        "tags": []
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=5>\n",
        "    <b>بسم الله الرحمن الرحیم</b>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4eabeea-4937-4e29-a3e8-f175352d4f14",
      "metadata": {
        "id": "b4eabeea-4937-4e29-a3e8-f175352d4f14",
        "tags": []
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=5>\n",
        "    <b>چالش درس هوش مصنوعی - پردازش زبان طبیعی</b>\n",
        "    <br>\n",
        "    <b>فاز اول</b>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "151f7b7a-ce4a-455a-b202-8185c744c764",
      "metadata": {
        "id": "151f7b7a-ce4a-455a-b202-8185c744c764",
        "tags": []
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "نوت بوکی که پیش روی شماست، مراحل انجام فاز اول چالش درس هوش مصنوعی در قسمت پردازش زبان طبیعی را تعیین کرده است. در این فاز، شما با استفاده مجموعه‌ای از محتوای متن صفحات وب به همراه برچسب موضوع هر یک، مدلی را توسعه می‌دهید که توانایی دسته‌بندی متن ورودی را به یکی از 16 برچسب مجموعه‌ داده داشته باشد.\n",
        "<br>\n",
        "<br>\n",
        "<b>چند نکته مهم</b>\n",
        "<ul >\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<li>\n",
        "می‌توانید از کتابخانه‌های با پیاده‌سازی آماده مدل‌های پایه (مانند transformers) استفاده کنید.\n",
        "</li>\n",
        "<li>\n",
        "توجه کنید که تمام کتابخانه های مورد نیاز را نصب کنید و از نصب شدن آن ها اطمینان حاصل کرده و نام آن ها در فایل requirements ذکر کنید.\n",
        "</li>\n",
        "<li>\n",
        "در صورت نیاز به نصب یک کتابخانه یا دسترسی به یک فایل، مراحل نصب و دانلود آن (از یک محل عمومی یا فایل ارسال شده به همراه نوت‌بوک) می‌بایست در نوت‌بوک وجود داشته باشد.\n",
        "</li>\n",
        "<li>\n",
        "تمامی فایل‌های مرتبط به پروژه که دارای حجم کمی هستند، باید به همراه نوت‌بوک در قالب یک فایل زیپ ارسال شوند. فایل‌هایی که حجم زیادی دارند (مانند فایل ذخیره شده یک مدل که بیشتر از ۲۰۰ مگابایت باشد) را در یک محل عمومی مانند گوگل درایو آپلود کرده و لینک دسترسی به آن را در نوت بوک قرار دهید.\n",
        "</li>\n",
        "</font>\n",
        "</ul>\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15621f21-c99f-4697-a374-280e29a8b1e4",
      "metadata": {
        "id": "15621f21-c99f-4697-a374-280e29a8b1e4"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<b>بارگذاری و پیش‌پردازش داده</b>\n",
        "<br>\n",
        "در ابتدا می‌بایست داده‌ها را از فایل با نام dataset_phase1 بارگذاری کنید. سپس برای اینکه بتوانید مدل مناسبی را آموزش دهید،  داده‌ها را پیش پردازش کنید. این قسمت می‌تواند شامل موارد متنوعی باشد؛ از جمله حذف عبارت‌های اضافی موجود در انتهای متن‌ها، حذف یا جداسازی علائم نگارشی، توکن‌بندی کلمات، جایگزینی مواردی مثل اعداد، تاریخ‌ها و آدرس‌‌های الکترونیکی با توکن‌های خاص، حذف فاصله‌های اضافه و ... . برای این منظور، می‌توانید از کتاب‌خانه‌هایی مانند hazm هم استفاده کنید.\n",
        "سپس برای اینکه بتوانید ارزیابی مناسبی از مدل خود داشته باشید، داده‌ها را به دو قسمت آموزش  (train) و ارزیابی  (validation) تقسیم کنید.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdbbd429-7ce5-42f4-842c-1639bb576b32",
      "metadata": {
        "id": "fdbbd429-7ce5-42f4-842c-1639bb576b32"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<b>آموزش و ارزیابی مدل</b>\n",
        "<br>\n",
        "در این قسمت شما به آماده‌سازی مدل انتخابی و آموزش دسته‌بند می‌پردازید. برای این‌که بتوانید به نتایج بهتری برسید، مقادیر مختلف پارامترها و ابرپارامترهای مدل را با استفاده از داده اعتبارسنجی ارزیابی کنید و بهترین مقادیر را انتخاب کنید. در ارزیابی مدل می‌توانید از معیارهای متفاوتی مانند\n",
        "      Recall ،Precision ،Accuracy ، Weighted F1-Scoreو F1-Score استفاده کنید.\n",
        "پس از انتخاب پارامترهای بهینه، می‌توانید مدل نهایی خود را روی تمامی داده‌ها آموزش دهید.\n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1495129c-21b2-4316-9f2b-b52255182123",
      "metadata": {
        "id": "1495129c-21b2-4316-9f2b-b52255182123"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<font face=\"B Nazanin\" size=4>\n",
        "<b>دسترسی به مدل</b>\n",
        "<br>\n",
        "برای ارزیابی مدل شما توسط داده آزمایش (test)، تابعی با نام classify_text بنویسید که به عنوان ورودی دیتافریم تست را دریافت کند و در خروجی یک لیست از اعداد فیلد 'class_id' (از جنس int) متناظر با متن دیتافریم ورودی در ردیف با همان index است را با توجه به موضوع متن تولید کند( test_dataframe.iloc[i] -> results[i] ). این تابع توسط دستیاران آموزشی روی داده‌های آزمایش اجرا شده و نتایج آن برای ارزیابی کیفیت مدل شما استفاده خواهد شد.  \n",
        "</font>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading the needed libraries and dataframe**"
      ],
      "metadata": {
        "id": "ljTrzj2iDy3_"
      },
      "id": "ljTrzj2iDy3_"
    },
    {
      "cell_type": "code",
      "source": [
        "# installing hazm\n",
        "!pip install hazm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from hazm import *"
      ],
      "metadata": {
        "id": "U0ya385lznLH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b2bc06-0e26-4259-bf82-e837bc57acf2"
      },
      "id": "U0ya385lznLH",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.25.0)\n",
            "Requirement already satisfied: python-crfsuite<0.10.0,>=0.9.9 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.9.9)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2CLzQf9RVDK",
        "outputId": "fd747bef-6541-4720-a896-8976d401ecbe"
      },
      "id": "f2CLzQf9RVDK",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "dataframe = pd.read_csv(\"/content/drive/MyDrive/phase_1_train.csv\")"
      ],
      "metadata": {
        "id": "rM82_XDBKzJh"
      },
      "id": "rM82_XDBKzJh",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n"
      ],
      "metadata": {
        "id": "l-Rh32o0oJwk"
      },
      "id": "l-Rh32o0oJwk",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import hazm\n",
        "\n",
        "normalizer = hazm.Normalizer()\n",
        "lemmatizer = hazm.Lemmatizer()\n",
        "def normalize_custom(line: str):\n",
        "    line = re.sub(r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    line = normalizer.normalize(line)\n",
        "    words = hazm.word_tokenize(line)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    line = ' '.join(words)\n",
        "    return line\n",
        "\n",
        "#dataset=dataframe[\"text\"]\n",
        "#for i in range(len(dataset)):\n",
        "#    dataset[i]=normalize_custom(dataset[i])\n",
        "\n",
        "#labelsa=dataframe.reset_index()[\"class_id\"]\n",
        "valid_dataset= dataframe.sample(400, replace=False)\n",
        "train_dataset= dataframe.drop(valid_dataset.index)\n",
        "train_queries=train_dataset.reset_index()[\"text\"]\n",
        "for i in range(len(train_queries)):\n",
        "  train_queries[i]=normalize_custom(train_queries[i])\n",
        "train_labels=np.array(train_dataset.reset_index()[\"class_id\"])\n"
      ],
      "metadata": {
        "id": "DOr53AIetoVN"
      },
      "id": "DOr53AIetoVN",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model\n",
        "import numpy as np\n",
        "\n",
        "valid_queries=valid_dataset.reset_index()[\"text\"]\n",
        "valid_labels=np.array(valid_dataset.reset_index()[\"class_id\"])\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "# Train the model using the training data\n",
        "model.fit(train_queries,train_labels)\n",
        "# Predict the categories of the test data\n",
        "predicted_categories = model.predict(valid_queries)\n",
        "\n",
        "print(\"The accuracy is {}\".format(accuracy_score(valid_labels, predicted_categories)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjNovsM6s6-M",
        "outputId": "e9a968bf-5e97-4928-c3cd-80c1444e6f62"
      },
      "id": "NjNovsM6s6-M",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 0.6425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename= \"model.pickle\"\n",
        "pickle.dump(model, open(filename, \"wb\"))\n"
      ],
      "metadata": {
        "id": "Tuc84RjH4XV2"
      },
      "id": "Tuc84RjH4XV2",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "vAri2KiHjaP9",
        "outputId": "f1b75e40-23eb-48c2-ad50-c5ce33663d76"
      },
      "id": "vAri2KiHjaP9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.9.0-py3-none-any.whl (477 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.8/477.8 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gensim<5.0.0,>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (4.3.1)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.8.1)\n",
            "Collecting numpy<2.0.0,>=1.24.3 (from hazm)\n",
            "  Downloading numpy-1.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-crfsuite<0.10.0,>=0.9.9 (from hazm)\n",
            "  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0.0,>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from hazm) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim<5.0.0,>=4.3.1->hazm) (6.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->hazm) (4.65.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2.0.0,>=1.2.2->hazm) (3.1.0)\n",
            "Installing collected packages: python-crfsuite, numpy, hazm\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed hazm-0.9.0 numpy-1.25.0 python-crfsuite-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import hazm\n",
        "import re\n",
        "\n",
        "class ClassificationModel():\n",
        "  def __init__(self):\n",
        "    filename= \"model.pickle\"\n",
        "    self.model=pickle.load(open(filename,\"rb\"))\n",
        "\n",
        "    self.normalizer = hazm.Normalizer()\n",
        "    self.lemmatizer = hazm.Lemmatizer()\n",
        "  def normalize_custom(self, line: str):\n",
        "      line = re.sub(r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
        "      line = re.sub(r'\\s+', ' ', line.strip())\n",
        "      line = self.normalizer.normalize(line)\n",
        "      words = hazm.word_tokenize(line)\n",
        "      words = [self.lemmatizer.lemmatize(word) for word in words]\n",
        "      line = ' '.join(words)\n",
        "      return line\n",
        "\n",
        "  def classify_text(self, test_dataframe):\n",
        "  # computation of the model’s output\n",
        "    test_queries=test_dataframe.reset_index()[\"text\"]\n",
        "    for i in range(len(test_queries)):\n",
        "      test_queries[i]=self.normalize_custom(test_queries[i])\n",
        "\n",
        "    return self.model.predict(test_queries)"
      ],
      "metadata": {
        "id": "c5VVzAN44xaf"
      },
      "id": "c5VVzAN44xaf",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeWBcDV_q1da"
      },
      "id": "eeWBcDV_q1da",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = ClassificationModel()\n",
        "valid_dataset= dataframe.sample(400, replace=False)\n",
        "results = classifier.classify_text(valid_dataset)\n",
        "#\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFmRXzYu5ySu",
        "outputId": "c8674801-523f-4f46-c355-e155c8554161"
      },
      "id": "jFmRXzYu5ySu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12 11 17 11 19 12 11 10 19 10 10 20 10 10 20 10 10 20 10 10 10 11 11 10\n",
            " 14 10 20 20  6 19 20 10 10 21 10 10 10 12 10 16 10 10  6  8 11 11 11 16\n",
            " 20 10 10 19  8 11 10 11 11  6 11 12 10 12 10 20 21 10 10 10  8 14 10 10\n",
            " 10 12 11 20 12 10 10 20 10 12 10 10 10 11  6 10 20 11 10 14  8  8 12 10\n",
            " 10 12 10  6 10 12 12 12 10 10 10 12 21 10 10 10 12 17 10 11 10 12 14 12\n",
            " 10 10  6 10 20 10 14 10 10 12 10 10 10 10 10 16 21 10 10 12 10  6 12 12\n",
            " 12 12 12 12 10 10 10 21 11 21 12 10 12 11 12 12 10 12 10 12 10 11 16 10\n",
            " 10 12 10 10 10 11 12 12 11 12 12 10 10 10 10  6 11 10 11 10 10 21 12 10\n",
            " 10 10 10 10 11 10 12 10  6 20 17 12 12  6 11 19 12 11 16 12 10 14 10 10\n",
            " 20 21 10 11 10 10 12 17 11 21 12 10 10 20  6 10 10 11 12 11 10 20 12 16\n",
            " 10 20 10 10 12 10 10 10 12 11 12 12 19 20 14 10 20  6 11 12 12 19 21 10\n",
            " 17  8 10 20 10 10 10 12 12 10 19  6 11 19 10 12 10 10 17 10 12  6 16  6\n",
            " 10 16 10 12 12 10 10 16 10 19 19 20 20 10 14 10 10 10 17 20 10 21 10 10\n",
            " 12 10 11 10 19 10 10 10 10 21  6 21 10 10 10 20 20  8  8 17 11 10 11 20\n",
            " 11 10 10 10 20 10 11 16 12 10 10  6 11 10 10 12 10 12 11 10 20 11  6 12\n",
            " 20 10 20 15 19 10 10 14 12 21 11 10 10 16  6 10 12 11 10  6 12 10 10 10\n",
            "  6 12 10 10 10 10  6 11 10 12 12 11 11 12  6  6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stWoH97jGqM-"
      },
      "id": "stWoH97jGqM-",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}